# Roadmap: Voice Cloning Project

## Цель проекта
Разработка системы синтеза речи на английском языке, имитирующей голос русского спикера. Цель — максимально естественное и точное клонирование голоса с возможностью управления темпом речи. Будет использоваться для генерации английских лекций на основе русского голоса.

## Изначальный подход (VITS) - Изменен
Изначально планировалось использовать модель **VITS** из Coqui TTS. Однако, после исследования выяснилось, что **отсутствует качественная, общедоступная предобученная базовая модель VITS для русского языка**, что делает fine-tuning для русской речи крайне затруднительным или требует обучения с нуля. Стандартный fine-tuning на английской модели (`en/vctk/vits`) потребовал бы английских транскрипций для русских аудио, что не соответствует цели сохранения оригинальной русской речи в данных.

## Текущий подход (XTTS v2)
В связи с ограничениями VITS для русского языка, было принято решение перейти на модель **XTTS v2** (`tts_models/multilingual/multi-dataset/xtts_v2`), также доступную в Coqui TTS. XTTS v2 поддерживает русский язык и подходит для многоязычных задач и клонирования голоса, включая zero-shot и few-shot сценарии.

### Опыт работы с AllTalk TTS
Для упрощения процесса fine-tuning XTTS v2 была опробована система **AllTalk TTS**. Однако на практике выявились ограничения:
- **Отсутствие автоматического продолжения обучения («resume»)** — необходимо вручную копировать чекпоинты;
- **Статичная подготовка датасета** — при любом изменении набор пересобирается полностью;
- **Fine-tune только акустической модели** — нет возможности дообучать вокодер;
- **Ограниченная настройка гиперпараметров через UI**;
- **Невозможность динамического добавления новых данных** без перезапуска всего процесса.

Несмотря на удачную начальную интеграцию и базовые результаты, эти ограничения не дали достаточной гибкости и контроля.

## Что сделано (на 2025-04-05)
- Создана рабочая директория проекта: `/ai/vits`
- Создано и используется conda-окружение: `venv_vits` (Python 3.10) в `/ai/vits/venv_vits`
- Клонирован репозиторий `coqui-ai/TTS` в `/ai/vits/src`
- Установлены зависимости и сам пакет TTS (editable mode)
- Установлен `espeak-ng`
- **Работа с AllTalk TTS для fine-tuning XTTS v2:**
    - Клонирован репозиторий AllTalk TTS в `/ai/vits/alltalk_tts`.
    - Создано и используется виртуальное окружение Python (`venv`) в `/ai/vits/alltalk_tts/alltalk_environment/env`.
    - Установлены основные зависимости (`transformers`, `num2words`, `TTS`, `gradio` версии 3.50.2).
    - Устранены ошибки, связанные с несовместимостью версий.
- **Модификация для работы на CPU:**
    - Создан скрипт `finetune_cpu.py` для обучения на CPU.
    - Добавлена логика принудительного использования CPU.
    - Модифицированы функции для игнорирования проверок GPU и адаптации к CPU.
- **Обучение модели на CPU:**
    - Выполнено обучение модели на CPU в течение 5 эпох.
    - Тестирование показало приемлемую реализацию fine-tuning на небольшом наборе данных.

## Текущие возможности
1. **Zero-shot синтез речи:**
   - Синтез английской речи с голосом русского спикера без обучения, используя только образец голоса.
2. **Fine-tuning акустической модели:**
   - Обучение на CPU и возможность продолжения обучения через перенос чекпоинтов.
3. **Веб-интерфейс тестирования:**
   - Простой UI для проверки результатов генерации.

## Инструкция для новых пользователей

- Для развертывания системы требуется только xtts-finetune-webui и его requirements.txt.
- Не устанавливайте дополнительные TTS- или ML-библиотеки вручную — используйте только зафиксированные версии из requirements.txt.
- После установки зависимостей:
    1. Откройте файл `faster_whisper/transcribe.py` (внутри вашего virtualenv/site-packages).
    2. Все параметры `total` и аргументы `pbar.update(...)` должны быть приведены к типу int (например, `int(content_duration)` и `int(n)`), чтобы избежать ошибок совместимости с gradio/pydantic.
    3. Запускайте xtts_demo.py стандартным способом из активированного окружения.
- Если появятся новые ошибки — проверьте совместимость зависимостей и повторите шаги по приведению к int.

## Внесённые доработки для запуска "из коробки"

### 1. Автоматическое разбиение длинных русских текстов (лимит XTTS)
- В файл `utils/formatter.py` добавлена функция, которая автоматически разбивает длинные строки текста на русском языке на сегменты не длиннее 180 символов при подготовке датасета.
- Это полностью устраняет ошибку "The text length exceeds the character limit of 182 for language 'ru'" при обучении и генерации аудио.
- Теперь любые длинные предложения будут корректно разбиваться для XTTS, что гарантирует отсутствие обрезки аудио и ошибок при fine-tuning.

### 2. Исправления для корректной работы зависимостей
- В `requirements.txt` обновлены версии пакетов (`numpy`, `tokenizers`, `ctranslate2`, `faster-whisper` и др.), чтобы исключить конфликты и ошибки при запуске на современных системах.
- Все зависимости теперь гарантированно совместимы между собой и с XTTS.

### 3. Локализация моделей и кешей
- Добавлен скрипт `set_project_env.sh`, который устанавливает переменные окружения так, чтобы все модели, веса и кеши скачивались и сохранялись строго внутри директории проекта (`/ai/xtts-finetune-webui/base_models`).
- Это предотвращает засорение домашней директории пользователя и гарантирует воспроизводимость среды.

**Как использовать скрипт:**
- Скрипт `set_project_env.sh` необходимо запускать (через `source`) каждый раз перед запуском xtts-finetune-webui, если вы хотите, чтобы все модели и кеши сохранялись локально в проекте:
  ```bash
  source ./set_project_env.sh
  python xtts_demo.py
  ```
- Если скрипт не запускать, модели и кеши будут сохраняться в домашней директории пользователя по умолчанию.

**Автоматизация через виртуальное окружение:**
- Чтобы переменные окружения устанавливались автоматически при каждом запуске виртуального окружения, добавьте строки из `set_project_env.sh` в файл активации окружения:
  - Для conda: добавьте их в файл `etc/conda/activate.d/env_vars.sh` внутри директории окружения.
  - Для venv/virtualenv: добавьте их в файл `bin/activate` (или создайте отдельный `postactivate`-скрипт).
- Пример для venv:
  ```bash
  # Вставьте в <venv>/bin/activate
  export TTS_CACHE_PATH="/ai/xtts-finetune-webui/base_models"
  export COQUI_TTS_HOME="/ai/xtts-finetune-webui/base_models"
  export HF_HOME="/ai/xtts-finetune-webui/base_models/huggingface"
  export XDG_CACHE_HOME="/ai/xtts-finetune-webui/base_models/xdg_cache"
  ```
- После этого переменные будут автоматически устанавливаться при каждом `source <venv>/bin/activate`, и запускать отдельный скрипт не потребуется.

### 4. Общая структура и рекомендации
- Все критичные изменения и доработки описаны в этом разделе Roadmap, чтобы любой новый пользователь мог развернуть проект и сразу приступить к работе без ручных исправлений.
- Проблемы, связанные с AllTalk TTS, больше не актуальны: весь функционал перенесён в xtts-finetune-webui.

## Дальнейшие шаги
1. **Переход на xtts-finetune-webui (ЗАВЕРШЕНО)**
   - Проект полностью переведён на `xtts-finetune-webui` для XTTS v2.
   - Для запуска системы теперь не требуется установка сторонних TTS-репозиториев или ручная настройка зависимостей — всё необходимое входит в xtts-finetune-webui.
   - Необходимо использовать только файл зависимостей `requirements.txt` из репозитория (он обновлён и гарантирует совместимость всех библиотек).
   - После установки зависимостей требуется вручную исправить баги в скрипте транскрипции:
     - В файле `faster_whisper/transcribe.py` все вызовы `tqdm(..., total=...)` и `pbar.update(...)` должны принимать целые значения (`int(...)`), чтобы избежать ошибок pydantic/gradio.
   - После этих изменений система запускается и работает "из коробки" без дополнительных правок.
   - Все шаги и проблемы, связанные с AllTalk TTS, больше не актуальны и не требуются.
2. **Подготовка качественного русского датасета**
   - Выбрать и скачать RUSLAN или RuLS (~30–100 ч) для начального fine-tuning.
   - Конвертировать и организовать аудио/метаданные (WAV 24kHz, mono).
3. **Fine-tuning acoustic model**
   - Провести обучение XTTS v2 на большом рассвернутом русском корпусе.
4. **Fine-tuning vocoder (опционально)**
   - После этапа 3 дообучить выбранный вокодер (HiFi-GAN или аналог) на своих мел-спектрограммах.
   - Это даст прирост в натуральности, плавности интонаций и сходстве с оригинальным голосом.
5. **Оптимизация гиперпараметров**
   - Эксперименты с learning rate, batch size, scheduler для улучшения качества (MOS).
6. **Контроль переобучения**
   - Интеграция EarlyStopping или мониторинговых инструментов (TensorBoard, WandB).
7. **Интеграция в конечную систему**
   - Экспорт и упаковка модели; разработка API или интерфейса для использования.

## Структура директорий
```
/ai/vits/
├── venv_vits/        # Conda окружение
├── src/              # Coqui TTS
│   ├── TTS/
│   ├── dataset/      # Оригинальный датасет (AllTalk TTS)
│   └── ...
├── alltalk_tts/      # Репозиторий AllTalk TTS
│   └── ...
├── docs/
│   └── Roadmap.md    # Этот файл
└── ...
```